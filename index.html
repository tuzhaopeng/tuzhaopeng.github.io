<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
	
<title>Zhaopeng Tu's Homepage</title>

<style type="text/css">
    body {
        font-family: cambria, calibri, garamond, century, gulim, dotum, arial;
        font-size: 18px;
        margin: 0 auto;
        width: 90%;
        min-width: 200px;
        max-width: 1200px;
        width: expression_r(Math.max(200, Math.min(1200, document.body.offsetWidth-40)) + "px");
    }
</style>

<style>
table {
    font-family: cambria, calibri, garamond, century, gulim, dotum, arial;
    border-collapse: collapse;
    width: 100%;
    font-size:18px;}
</style>

</head>
<body>


<table class="wsite-multicol-table">
<tbody class="wsite-multicol-tbody">
<tr class="wsite-multicol-tr">

<td class="wsite-multicol-col">
    <img src="images/head.jpg" height=204.75 width=307.2 border="0"  align="center">

    <br/>
    <h2>Zhaopeng Tu</h2>

    <p>Principal Researcher<br />
    Hunyuan AI Digital Human, Tencent<br />
    tuzhaopeng@gmail.com</p>
</td>

</tr>
</tbody>
</table>


<h2>Bio</h2>
<p>Zhaopeng Tu is a principal researcher at Tencent AI Lab, whose research focuses on deep learning for natural language processing (NLP). 
He is currently working on neural machine translation (NMT) and large language modeling (LLM). 
He has published over 100 papers in leading NLP/AI journals and conferences such as ICML, ACL, EMNLP, ICLR, and TACL. 
He served as Associate Editor of NeuroComputing, Area Chair or Senior PC Member of ACL, EMNLP, NAACL, AAAI, and IJCAI.</p>

<!--p></p>
<center><font color="#008F00">Please refer to <a href="intern.html">research intern positions</a> for the information about internship.</font></center-->

<h2>News</h2> 

<p></p>

<ul>
  <li type="circle">2025-05: 2 papers were accepted to ICML2025.</li>        
  <li type="circle">2025-01: 3 papers were accepted to ICLR2025.</li>        
  <li type="circle">2024-10: Our paper titled "<i>GPT4Video: A Unified Multimodal Large Language Model for lnstruction-Followed Understanding and Safety-Aware Generation</i>" was <font color="#C4260B"><b>nominated for the Best Paper Award</b></font> at ACM MM 2024.</li>        
  <li type="circle">2024-10: 1 paper was accepted to Transactions of ACL, and 1 paper was accepted to IEEE/ACM Transactions on Audio, Speech, and Language Processing.</li>        
  <li type="circle">2024-09: 1 paper was accepted to EMNLP2024, and 3 papers (including 2 D&B Papers) were accepted to NeurIPS2024.</li>        
  <li type="circle">2024-07: 1 paper was accepted to ACM-MM2024, and 2 papers were accepted to ECAI2024.</li>        
  <li type="circle">2024-05: 1 paper was accepted to ICML2024, and 6 papers were accepted to ACL2024.</li>    
  <li type="circle">2024-01: 2 papers were accepted to ICLR2024.</li>    
  <li type="circle">2023-10: 3 papers were accepted to EMNLP2023.</li>    
  <li type="circle">2023-05: 2 papers were accepted to ACL2023.</li>    
  <li type="circle">2022-11: Our paper titled "<i>Adapters for Enhanced Modeling of Multilingual Knowledge and Text</i>" was selected as the <font color="#C4260B"><b>Best Paper Award</b></font> at MRL Workshop in EMNLP2022.</li>    
  <li type="circle">2022-10: 3 papers were accepted to EMNLP2022.</li>    
  <li type="circle">2022-02: 3 papers were accepted to ACL2022.</li>    
</ul>

<p></p>


<!-- h2>Selected Talks</h2>

<ul>
  <li type="circle">07/29/2019: A talk on Tencent Academic and Industrial Conference (TAIC) in ACL2019, which summarizes our recent work in machine translation. [<a href="talks/20190729 Information Transformation.pdf"><font color="#0080FF">slides</font></a>]</li>
  <li type="circle">07/27/2018: A lecture in CIPS Summer School, which gives a survey of recent progresses on improving NMT models (covering 70 papers). [<a href="talks/20180727_CIPS_Summer_School.pdf"><font color="#0080FF">slides (in Chinese)</font></a>]
  <li type="circle">01/04/2018: A talk on <i>Adequacy-Oriented NMT</i>, which summarizes our recent papers that were accepted in the last year. [<a href="talks/adequacy-oriented_nmt.pdf"><font color="#0080FF">slides</font></a>] [<a href="https://mp.weixin.qq.com/s/joZnfPtR3vv6LJPsRrY9Jw"><font color="#0080FF">video and textual explication (in Chinese)</font></a>]</li>
</ul>

<p></p-->


<h2>Selected Publications</h2> 

<a href="publications.html">full list</a>

<p></p>
<h3>Long Reasoning Models</h3>
<ul>
    <li type="circle">Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, Rui Wang, <u>Zhaopeng Tu</u>*, Haitao Mi, and Dong Yu. <a href="https://arxiv.org/abs/2412.21187"><font color="#0080FF">Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs</font></a>. <i>ICML 2025</i>. <a href="https://x.com/tuzhaopeng/status/1873924882012291463">X</a></li>
    <li type="circle">Yue Wang, Qiuzhi Liu, Jiahao Xu, Tian Liang, Xingyu Chen, Zhiwei He, Linfeng Song, Dian Yu, Juntao Li, Zhuosheng Zhang, Rui Wang, <u>Zhaopeng Tu</u>*, Haitao Mi, and Dong Yu. <a href="https://arxiv.org/abs/2501.18585"><font color="#0080FF">Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs</font></a>. <i>Preprint</i>. <a href="https://x.com/tuzhaopeng/status/1885179412163027406">X</a></li>
    <li type="circle">Zicheng Lin, Tian Liang, Jiahao Xu, Qiuzhi Liu, Xing Wang, Ruilin Luo, Chufan Shi, Siheng Li, Yujiu Yang, and <u>Zhaopeng Tu</u>*. <a href="https://arxiv.org/abs/2411.19943"><font color="#0080FF">Critical Tokens Matter: Token-Level Contrastive Estimation Enhances LLM's Reasoning Capability</font></a>. <i>ICML 2025</i>. <a href="https://x.com/tuzhaopeng/status/1879065390992753149">X</a></li>
    <li type="circle">Ante Wang, Linfeng Song, Ye Tian, Dian Yu, Haitao Mi, Xiangyu Duan, <u>Zhaopeng Tu</u>*, Jinsong Su, and Dong Yu. <a href="https://arxiv.org/abs/2502.11183"><font color="#0080FF">Don't Get Lost in the Trees: Streamlining LLM Reasoning by Overcoming Tree Search Exploration Pitfalls</font></a>. <i>Preprint</i>. <a href="https://x.com/tuzhaopeng/status/1891346931433255300">X</a></li>
    <li type="circle">Ke Ji, Jiahao Xu, Tian Liang, Qiuzhi Liu, Zhiwei He, Xingyu Chen, Xiaoyuan Liu, Zhijie Wang, Junying Chen, Benyou Wang, <u>Zhaopeng Tu</u>*, Haitao Mi, and Dong Yu. <a href="http://dx.doi.org/10.13140/RG.2.2.33772.07043"><font color="#0080FF">The First Few Tokens Are All You Need: An Efficient and Effective Unsupervised Prefix Fine-Tuning Method for Reasoning Models</font></a>. <i>Preprint</i>. <a href="https://x.com/tuzhaopeng/status/1895420224683516413">X</a></li>	
</ul>

<h3>Large Language Models</h3>
<ul>	
    <li type="circle">Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Xing Wang, and <u>Zhaopeng Tu</u>. <a href="https://arxiv.org/abs/2301.08745"><font color="#0080FF">Is ChatGPT A Good Translator? Yes With GPT-4 As The Engine</font></a>. <i>Preprint</i>.</li>

    <li type="circle">Zhanyu Wang, Longyue Wang, Zhen Zhao, Minghao Wu, Chenyang Lyu, Huayang Li, Deng Cai, Luping Zhou, Shuming Shi, and <u>Zhaopeng Tu</u>. <a href="https://arxiv.org/abs/2311.16511"><font color="#0080FF">GPT4Video: A Unified Multimodal Large Language Model for lnstruction-Followed Understanding and Safety-Aware Generation</font></a>. <i>ACM MM 2024</i>. [<font color="#C4260B"><b>Best Paper Normination</b></font>]</li>

    <li type="circle">Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Shuming Shi, and <u>Zhaopeng Tu</u>. <a href="https://arxiv.org/abs/2305.19118"><font color="#0080FF">Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate</font></a>. <i>EMNLP 2024</i>.</li>
    
    <li type="circle">Wenxuan Wang, Wenxiang Jiao, Jingyuan Huang, Ruyi Dai, Jen-tse Huang, <u>Zhaopeng Tu</u>*, and Michael R Lyu. <a href="https://arxiv.org/abs/2310.12481"><font color="#0080FF">Not All Countries Celebrate Thanksgiving: On the Cultural Dominance in Large Language Models</font></a>. <i>ACL 2024</i>.</li>

    <li type="circle">Jen-tse Huang, Man Ho LAM, Eric John Li, Shujie Ren, Wenxuan Wang, Wenxiang Jiao, <u>Zhaopeng Tu</u>, and Michael Lyu. <a href="https://openreview.net/forum?id=pwRVGRWtGg"><font color="#0080FF">Apathetic or Empathetic? Evaluating LLMs' Emotional Alignments with Humans</font></a>. <i>NeurIPS 2024</i>.</li>
    <li type="circle">Jen-tse Huang, Wenxuan Wang, Eric John Li, Man Ho LAM, Shujie Ren, Youliang Yuan, Wenxiang Jiao, <u>Zhaopeng Tu</u>, and Michael Lyu. <a href="https://openreview.net/forum?id=H3UayAQWoE"><font color="#0080FF">On the Humanity of Conversational AI: Evaluating the Psychological Portrayal of LLMs</font></a>. <i>ICLR 2024</i> (Oral, 1.2%).</li>
        
    <li type="circle">Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Jiahao Xu, Tian Liang, Pinjia He, and <u>Zhaopeng Tu</u>. <a href="https://arxiv.org/abs/2407.09121"><font color="#0080FF">Refuse Whenever You Feel Unsafe: Improving Safety in LLMs via Decoupled Refusal Training</font></a>. <i>Preprint</i>. <a href="https://x.com/youliang_yuan/status/1812665889852121332">X</a></li>
    <li type="circle">Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Pinjia He, Shuming Shi, and <u>Zhaopeng Tu</u>. <a href="https://arxiv.org/abs/2308.06463"><font color="#0080FF">GPT-4 is too Smart to be Safe: Stealthy Chat with LLMs via Cipher</font></a>. <i>ICLR 2024</i>.</li>
    
    <li type="circle">Zhiwei He, Tian Liang, Wenxiang Jiao, Zhuosheng Zhang, Yujiu Yang, Rui Wang, <u>Zhaopeng Tu</u>*, Shuming Shi, and Xing Wang. <a href="https://arxiv.org/abs/2305.04118"><font color="#0080FF">Exploring Human-Like Translation Strategy with Large Language Models</font></a>. <i>TACL 2024</i>.</li>
    
    <li type="circle">Zhiwei He, Xing Wang, Wenxiang Jiao, Zhuosheng Zhang, Rui Wang, Shuming Shi, and <u>Zhaopeng Tu</u>. <a href="https://arxiv.org/abs/2401.12873"><font color="#0080FF">Improving Machine Translation with Human Feedback: An Exploration of Quality Estimation as a Reward Model</font></a>. <i>NAACL 2024</i>.</li>
    
    <li type="circle">Longyue Wang, Chenyang Lyu, Tianbo Ji, Zhirui Zhang, Dian Yu, Shuming Shi, and <u>Zhaopeng Tu</u>. <a href="https://arxiv.org/pdf/2304.02210.pdf"><font color="#0080FF">Document-Level Machine Translation with Large Language Models</font></a>. <i>EMNLP 2023</i>.</li>
    <li type="circle">Wenxiang Jiao, Jen-tse Huang, Wenxuan Wang, Xing Wang, Shuming Shi, and <u>Zhaopeng Tu</u>. <a href="https://arxiv.org/abs/2304.02426"><font color="#0080FF">ParroT: Translating During Chat Using Large Language Models</font></a>. <i>EMNLP 2023 (Findings)</i>.</li>
    <li type="circle">Yifan Hou, Wenxiang Jiao, Meizhen Liu, Carl Allen, <u>Zhaopeng Tu</u>, and Mrinmaya Sachan. <font color="#0080FF">Adapters for Enhanced Modeling of Multilingual Knowledge and Text</font>. <i>EMNLP 2022 (Findings)</i>. [<font color="#C4260B"><b>Best Paper of MRL Workshop</b></font>]</li>

</ul>


<h3>Neural Machine Translation</h3>
<ul>
    <li type="circle"><u>Zhaopeng Tu</u>, Zhengdong Lu, Yang Liu, Xiaohua Liu, and Hang Li. <a href="http://arxiv.org/abs/1601.04811"><font color="#0080FF">Modeling Coverage for Neural Machine Translation</font></a>. <i>ACL 2016</i>. [<a href="https://github.com/tuzhaopeng/NMT-Coverage">code</a>]</li>
    <li type="circle"><u>Zhaopeng Tu</u>, Yang Liu, Zhengdong Lu, Xiaohua Liu, and Hang Li. <a href="http://arxiv.org/abs/1608.06043"><font color="#0080FF">Context Gates for Neural Machine Translation</font></a>. <i>TACL 2017</i>. [<a href="https://github.com/tuzhaopeng/NMT">code</a>]</li>
    <li type="circle"><u>Zhaopeng Tu</u>, Yang Liu, Lifeng Shang, Xiaohua Liu, and Hang Li. <a href="https://arxiv.org/abs/1611.01874"><font color="#0080FF">Neural Machine Translation with Reconstruction</font></a>. <i>AAAI 2017</i>. [<a href="https://github.com/tuzhaopeng/NMT">code</a>]</li>
    <li type="circle"><u>Zhaopeng Tu</u>, Yang Liu, Shuming Shi, and Tong Zhang. <a href="https://arxiv.org/abs/1711.09367"><font color="#0080FF">Learning to Remember Translation History with a Continuous Cache</font></a>. <i>TACL 2018</i>. </li>
</ul>
<p></p>





<!--p></p>

<h2>Selected Professional Services</h2>

<p></p>

<h3>Journals</h3>
<ul>
    <li type="circle">NeuroComputing: Associate Editor (2020-)</li>
    <li type="circle">Computational Linguistics: Reviewer (2016-)</li>
</ul>

<h3>Conferences</h3>
<ul>
    <li type="circle">Virtual Infrastracture Chair: EMNLP (2021)</li>
    <li type="circle">Area Chair: ACL (2019,2023), EMNLP (2018-2019), NAACL (2019)</li>
    <li type="circle">Senior Program Committee: AAAI (2019), IJCAI (2021)</li>
</ul-->

</body>
</html>
