<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
	
<title>Zhaopeng Tu's Homepage</title>

<style type="text/css">
    body {
        font-family: cambria, calibri, garamond, century, gulim, dotum, arial;
        font-size: 18px;
        margin: 0 auto;
        width: 90%;
        min-width: 200px;
        max-width: 1200px;
        width: expression_r(Math.max(200, Math.min(1200, document.body.offsetWidth-40)) + "px");
    }
</style>

<style>
table {
    font-family: cambria, calibri, garamond, century, gulim, dotum, arial;
    border-collapse: collapse;
    width: 100%;
    font-size:18px;}
</style>

</head>
<body>


<table class="wsite-multicol-table">
<tbody class="wsite-multicol-tbody">
<tr class="wsite-multicol-tr">

<td class="wsite-multicol-col">
    <img src="images/head.jpg" height=204.75 width=307.2 border="0"  align="center">

    <br/>
    <h2>Zhaopeng Tu</h2>

    <p>Principal Researcher<br />
    <a href="http://ai.tencent.com/ailab/">Tencent AI Lab</a><br />
    tuzhaopeng@gmail.com</p>
</td>

</tr>
</tbody>
</table>


<h2>Bio</h2>
<p>Zhaopeng Tu is a principal researcher at Tencent AI Lab, whose research focuses on deep learning for natural language processing (NLP). 
He is currently working on neural machine translation (NMT) and large language modeling (LLM). 
He has published over 100 papers in leading NLP/AI journals and conferences such as ICML, ACL, EMNLP, ICLR, and TACL. 
He served as Associate Editor of NeuroComputing, Area Chair or Senior PC Member of ACL, EMNLP, NAACL, AAAI, and IJCAI.</p>

<!--p></p>
<center><font color="#008F00">Please refer to <a href="intern.html">research intern positions</a> for the information about internship.</font></center-->

<h2>News</h2> 

<p></p>

<ul>
  <li type="circle">2024-10: Our paper titled "<i>GPT4Video: A Unified Multimodal Large Language Model for lnstruction-Followed Understanding and Safety-Aware Generation</i>" was <font color="#C4260B"><b>nominated for the Best Paper Award</b></font> at ACM MM 2024.</li>        
  <li type="circle">2024-10: 1 paper was accepted to Transactions of ACL, and 1 paper was accepted to IEEE/ACM Transactions on Audio, Speech, and Language Processing.</li>        
  <li type="circle">2024-09: 1 paper was accepted to EMNLP2024, and 3 papers (including 2 D&B Papers) were accepted to NeurIPS2024.</li>        
  <li type="circle">2024-07: 1 paper was accepted to ACM-MM2024, and 2 papers were accepted to ECAI2024.</li>        
  <li type="circle">2024-05: 1 paper was accepted to ICML2024, and 6 papers were accepted to ACL2024.</li>    
  <li type="circle">2024-01: 2 papers were accepted to ICLR2024.</li>    
  <li type="circle">2023-10: 3 papers were accepted to EMNLP2023.</li>    
  <li type="circle">2023-05: 2 papers were accepted to ACL2023.</li>    
  <li type="circle">2022-11: Our paper titled "<i>Adapters for Enhanced Modeling of Multilingual Knowledge and Text</i>" was selected as the <font color="#C4260B"><b>Best Paper Award</b></font> at MRL Workshop in EMNLP2022.</li>    
  <li type="circle">2022-10: 3 papers were accepted to EMNLP2022.</li>    
  <li type="circle">2022-02: 3 papers were accepted to ACL2022.</li>    
</ul>

<p></p>


<!-- h2>Selected Talks</h2>

<ul>
  <li type="circle">07/29/2019: A talk on Tencent Academic and Industrial Conference (TAIC) in ACL2019, which summarizes our recent work in machine translation. [<a href="talks/20190729 Information Transformation.pdf"><font color="#0080FF">slides</font></a>]</li>
  <li type="circle">07/27/2018: A lecture in CIPS Summer School, which gives a survey of recent progresses on improving NMT models (covering 70 papers). [<a href="talks/20180727_CIPS_Summer_School.pdf"><font color="#0080FF">slides (in Chinese)</font></a>]
  <li type="circle">01/04/2018: A talk on <i>Adequacy-Oriented NMT</i>, which summarizes our recent papers that were accepted in the last year. [<a href="talks/adequacy-oriented_nmt.pdf"><font color="#0080FF">slides</font></a>] [<a href="https://mp.weixin.qq.com/s/joZnfPtR3vv6LJPsRrY9Jw"><font color="#0080FF">video and textual explication (in Chinese)</font></a>]</li>
</ul>

<p></p-->


<h2>Selected Publications</h2> 

<a href="publications.html">full list</a>

<p></p>


<h3>Large Language Models</h3>
<ul>
    <li type="circle">Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Xing Wang, and <u>Zhaopeng Tu</u>. <a href="https://arxiv.org/abs/2301.08745"><font color="#0080FF">Is ChatGPT A Good Translator? Yes With GPT-4 As The Engine</font></a>. <i>Preprint</i>.</li>

    <li type="circle">Zhanyu Wang, Longyue Wang, Zhen Zhao, Minghao Wu, Chenyang Lyu, Huayang Li, Deng Cai, Luping Zhou, Shuming Shi, and <u>Zhaopeng Tu</u>. <a href="https://arxiv.org/abs/2311.16511"><font color="#0080FF">GPT4Video: A Unified Multimodal Large Language Model for lnstruction-Followed Understanding and Safety-Aware Generation</font></a>. <i>ACM MM 2024</i>. [<font color="#C4260B"><b>Best Paper Normination</b></font>]</li>

    <li type="circle">Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Shuming Shi, and <u>Zhaopeng Tu</u>. <a href="https://arxiv.org/abs/2305.19118"><font color="#0080FF">Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate</font></a>. <i>EMNLP 2024</i>.</li>

    <li type="circle">Cunxiao Du, Jing Jiang, Xu Yuanchen, Jiawei Wu, Sicheng Yu, Yongqi Li, Shenggui Li, Kai Xu, Liqiang Nie, <u>Zhaopeng Tu</u>*, and Yang You. <a href="https://arxiv.org/abs/2402.02082"><font color="#0080FF">GliDe with a CaPE: A Low-Hassle Method to Accelerate Speculative Decoding</font></a>. <i>ICML 2024</i>.</li>
    <li type="circle">Jen-tse Huang, Man Ho LAM, Eric John Li, Shujie Ren, Wenxuan Wang, Wenxiang Jiao, <u>Zhaopeng Tu</u>, and Michael Lyu. <a href="https://openreview.net/forum?id=pwRVGRWtGg"><font color="#0080FF">Apathetic or Empathetic? Evaluating LLMs' Emotional Alignments with Humans</font></a>. <i>NeurIPS 2024</i>.</li>


    <li type="circle">Wenxuan Wang, Wenxiang Jiao, Jingyuan Huang, Ruyi Dai, Jen-tse Huang, <u>Zhaopeng Tu</u>*, and Michael R Lyu. <a href="https://arxiv.org/abs/2310.12481"><font color="#0080FF">Not All Countries Celebrate Thanksgiving: On the Cultural Dominance in Large Language Models</font></a>. <i>ACL 2024</i>.</li>
    <li type="circle">Zhiwei He, Binglin Zhou, Hongkun Hao, Aiwei Liu, Xing Wang, <u>Zhaopeng Tu</u>, Zhuosheng Zhang, and Rui Wang. <font color="#0080FF">On the Cross-lingual Consistency of Text Watermark for Large Language Models</font>. <i>ACL 2024</i>.</li>
    <li type="circle">Wenxuan Wang, <u>Zhaopeng Tu</u>, Chang Chen, Youliang Yuan, Jen-tse Huang, Wenxiang Jiao, and Michael R Lyu. <a href="https://arxiv.org/abs/2310.00905"><font color="#0080FF">All Languages Matter: On the Multilingual Safety of Large Language Models</font></a>. <i>ACL 2024 (Findings)</i>.</li>
    <li type="circle">Zefeng Du, Wenxiang Jiao, Longyue Wang, Chenyang Lyu, Jianhui Pang, Leyang Cui, Kaiqiang Song, Derek F. Wong, Shuming Shi, and <u>Zhaopeng Tu</u>. <font color="#0080FF">Benchmarking and Improving Long-Text Translation with Large Language Models</font>. <i>ACL 2024 (Findings)</i>.</li>
    
    <li type="circle">Jen-tse Huang, Wenxuan Wang, Eric John Li, Man Ho LAM, Shujie Ren, Youliang Yuan, Wenxiang Jiao, <u>Zhaopeng Tu</u>, and Michael Lyu. <a href="https://openreview.net/forum?id=H3UayAQWoE"><font color="#0080FF">On the Humanity of Conversational AI: Evaluating the Psychological Portrayal of LLMs</font></a>. <i>ICLR 2024</i> (Oral, 1.2%).</li>

    <li type="circle">Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Pinjia He, Shuming Shi, and <u>Zhaopeng Tu</u>. <a href="https://arxiv.org/abs/2308.06463"><font color="#0080FF">GPT-4 is too Smart to be Safe: Stealthy Chat with LLMs via Cipher</font></a>. <i>ICLR 2024</i>.</li>
    
    <li type="circle">Zhiwei He, Tian Liang, Wenxiang Jiao, Zhuosheng Zhang, Yujiu Yang, Rui Wang, <u>Zhaopeng Tu</u>*, Shuming Shi, and Xing Wang. <a href="https://arxiv.org/abs/2305.04118"><font color="#0080FF">Exploring Human-Like Translation Strategy with Large Language Models</font></a>. <i>TACL 2024</i>.</li>
    
    <li type="circle">Zhiwei He, Xing Wang, Wenxiang Jiao, Zhuosheng Zhang, Rui Wang, Shuming Shi, and <u>Zhaopeng Tu</u>. <a href="https://arxiv.org/abs/2401.12873"><font color="#0080FF">Improving Machine Translation with Human Feedback: An Exploration of Quality Estimation as a Reward Model</font></a>. <i>NAACL 2024</i>.</li>
    
    <li type="circle">Longyue Wang, Chenyang Lyu, Tianbo Ji, Zhirui Zhang, Dian Yu, Shuming Shi, and <u>Zhaopeng Tu</u>. <a href="https://arxiv.org/pdf/2304.02210.pdf"><font color="#0080FF">Document-Level Machine Translation with Large Language Models</font></a>. <i>EMNLP 2023</i>.</li>
    <li type="circle">Wenxiang Jiao, Jen-tse Huang, Wenxuan Wang, Xing Wang, Shuming Shi, and <u>Zhaopeng Tu</u>. <a href="https://arxiv.org/abs/2304.02426"><font color="#0080FF">ParroT: Translating During Chat Using Large Language Models</font></a>. <i>EMNLP 2023 (Findings)</i>.</li>
</ul>


<h3>Autoregreesive NMT</h3>
<ul>
    <li type="circle"><u>Zhaopeng Tu</u>, Zhengdong Lu, Yang Liu, Xiaohua Liu, and Hang Li. <a href="http://arxiv.org/abs/1601.04811"><font color="#0080FF">Modeling Coverage for Neural Machine Translation</font></a>. <i>ACL 2016</i>. [<a href="https://github.com/tuzhaopeng/NMT-Coverage">code</a>]</li>
    <li type="circle"><u>Zhaopeng Tu</u>, Yang Liu, Zhengdong Lu, Xiaohua Liu, and Hang Li. <a href="http://arxiv.org/abs/1608.06043"><font color="#0080FF">Context Gates for Neural Machine Translation</font></a>. <i>TACL 2017</i>. [<a href="https://github.com/tuzhaopeng/NMT">code</a>]</li>
    <li type="circle"><u>Zhaopeng Tu</u>, Yang Liu, Lifeng Shang, Xiaohua Liu, and Hang Li. <a href="https://arxiv.org/abs/1611.01874"><font color="#0080FF">Neural Machine Translation with Reconstruction</font></a>. <i>AAAI 2017</i>. [<a href="https://github.com/tuzhaopeng/NMT">code</a>]</li>
    <li type="circle">Longyue Wang, <u>Zhaopeng Tu</u>*, Andy Way, and Qun Liu. <a href="https://arxiv.org/abs/1704.04347"><font color="#0080FF">Exploiting Cross-Sentence Context for Neural Machine Translation</font></a>. <i>EMNLP 2017 (Short)</i>. </li>
    <li type="circle"><u>Zhaopeng Tu</u>, Yang Liu, Shuming Shi, and Tong Zhang. <a href="https://arxiv.org/abs/1711.09367"><font color="#0080FF">Learning to Remember Translation History with a Continuous Cache</font></a>. <i>TACL 2018</i>. </li>
</ul>

<h3>Non-Autoregreesive NMT</h3>
<ul>
    <li type="circle">Cunxiao Du, <u>Zhaopeng Tu</u>*, and Jing Jiang. <a href="https://arxiv.org/abs/2106.05093"><font color="#0080FF">Order-Agnostic Cross Entropy for Non-Autoregressive Machine Translation</font></a>. <i>ICML 2021</i> (Oral, 1.2%). </li>
    <li type="circle">Cunxiao Du, <u>Zhaopeng Tu</u>*, Longyue Wang, and Jing Jiang. <a href=""><font color="#0080FF">ngram-OAXE: Phrase-Based Order-Agnostic Cross Entropy for Non-Autoregressive Machine Translation</font></a>. <i>COLING 2022</i>. </li>


    <li type="circle">Liang Ding, Longyue Wang, Xuebo Liu, Derek F. Wong, Dacheng Tao, and <u>Zhaopeng Tu</u>. <a href="https://openreview.net/forum?id=ZTFeSBIX9C"><font color="#0080FF">Understanding and Improving Lexical Choice in Non-Autoregressive Translation</font></a>. <i>ICLR 2021</i>. </li>
    <li type="circle">Liang Ding, Longyue Wang, Xuebo Liu, Derek F. Wong, Dacheng Tao, and <u>Zhaopeng Tu</u>. <a href="https://arxiv.org/abs/2106.00903"><font color="#0080FF">Rejuvenating Low-Frequency Words: Making the Most of Parallel Data in Non-Autoregressive Translation</font></a>. <i>ACL 2021</i>. </li>
    <li type="circle">Liang Ding, Longyue Wang, Shuming Shi, Dacheng Tao, and <u>Zhaopeng Tu</u>. <a href="https://openreview.net/forum?id=Zh4kJ4JOcHo"><font color="#0080FF">Redistributing Low-Frequency Words: Making the Most of Monolingual Data in Non-Autoregressive Translation</font></a>. <i>ACL 2022</i>. </li>
</ul>

<p></p>





<!--p></p>

<h2>Selected Professional Services</h2>

<p></p>

<h3>Journals</h3>
<ul>
    <li type="circle">NeuroComputing: Associate Editor (2020-)</li>
    <li type="circle">Computational Linguistics: Reviewer (2016-)</li>
</ul>

<h3>Conferences</h3>
<ul>
    <li type="circle">Virtual Infrastracture Chair: EMNLP (2021)</li>
    <li type="circle">Area Chair: ACL (2019,2023), EMNLP (2018-2019), NAACL (2019)</li>
    <li type="circle">Senior Program Committee: AAAI (2019), IJCAI (2021)</li>
</ul-->

</body>
</html>
