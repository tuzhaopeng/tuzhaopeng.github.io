<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
	
<title>Zhaopeng Tu's Publications</title>

<style type="text/css">
    body {
        font-family: cambria, calibri, garamond, century, gulim, dotum, arial;
        font-size: 18px;
        margin: 0 auto;
        width: 90%;
        min-width: 200px;
        max-width: 1200px;
        width: expression_r(Math.max(200, Math.min(1200, document.body.offsetWidth-40)) + "px");
    }
</style>

<style>
table {
    font-family: cambria, calibri, garamond, century, gulim, dotum, arial;
    border-collapse: collapse;
    width: 100%;
    font-size:18px;}
</style>

</head>
<body>



<!-- img border="0" height="276.3" width="337.5" src="images/zptu.jpg">

<h2><b>Zhaopeng Tu</b></h2>
<p align="left">
<b>Principal Researcher</b><br/>
Hunyuan AI Digital Human, Tencent<br />
tuzhaopeng@gmail.com
</p -->

<table class="wsite-multicol-table">
<tbody class="wsite-multicol-tbody">
<tr class="wsite-multicol-tr">

<td class="wsite-multicol-col">
    <img src="images/head.jpg" height=204.75 width=307.2 border="0"  align="center">


    <br/>
    <h2>Zhaopeng Tu</h2>

    <p>Principal Researcher<br />
    <a href="http://ai.tencent.com/ailab/">Tencent AI Lab</a><br />
    tuzhaopeng@gmail.com</p>
</td>

</tr>
</tbody>
</table>


<h2>Bio</h2>
<p>Zhaopeng Tu is a principal researcher at Tencent AI Lab, whose research focuses on deep learning for natural language processing (NLP). 
He is currently working on neural machine translation (NMT) and large language modeling (LLM). 
He has published over 100 papers in leading NLP/AI journals and conferences such as ICML, ACL, EMNLP, ICLR, and TACL. 
He served as Associate Editor of NeuroComputing, Area Chair or Senior PC Member of ACL, EMNLP, NAACL, AAAI, and IJCAI.</p>

<h2>Publications</h2> 
<a href="https://scholar.google.com/citations?user=IvE2zRgAAAAJ">Google Scholar Citations</a> 
<br/><br/>

(* denotes corresponding author.)



<ol reversed>
<h3>Preprint</h3>
    <li>Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, Rui Wang, <u>Zhaopeng Tu</u>*, Haitao Mi, and Dong Yu. <a href="https://arxiv.org/abs/2412.21187"><font color="#0080FF">Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs</font></a>. <i>ICML</i>. <a href="https://x.com/tuzhaopeng/status/1873924882012291463">X</a></li>
    <li>Yue Wang, Qiuzhi Liu, Jiahao Xu, Tian Liang, Xingyu Chen, Zhiwei He, Linfeng Song, Dian Yu, Juntao Li, Zhuosheng Zhang, Rui Wang, <u>Zhaopeng Tu</u>*, Haitao Mi, and Dong Yu. <a href="https://arxiv.org/abs/2501.18585"><font color="#0080FF">Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs</font></a>. <i>Preprint</i>. <a href="https://x.com/tuzhaopeng/status/1885179412163027406">X</a></li>
    <li>Zicheng Lin, Tian Liang, Jiahao Xu, Qiuzhi Liu, Xing Wang, Ruilin Luo, Chufan Shi, Siheng Li, Yujiu Yang, and <u>Zhaopeng Tu</u>*. <a href="https://arxiv.org/abs/2411.19943"><font color="#0080FF">Critical Tokens Matter: Token-Level Contrastive Estimation Enhances LLM's Reasoning Capability</font></a>. <i>ICML</i>. <a href="https://x.com/tuzhaopeng/status/1879065390992753149">X</a></li>
    <li>Ante Wang, Linfeng Song, Ye Tian, Dian Yu, Haitao Mi, Xiangyu Duan, <u>Zhaopeng Tu</u>*, Jinsong Su, and Dong Yu. <a href="https://arxiv.org/abs/2502.11183"><font color="#0080FF">Don't Get Lost in the Trees: Streamlining LLM Reasoning by Overcoming Tree Search Exploration Pitfalls</font></a>. <i>Preprint</i>. <a href="https://x.com/tuzhaopeng/status/1891346931433255300">X</a></li>
    <li>Ke Ji, Jiahao Xu, Tian Liang, Qiuzhi Liu, Zhiwei He, Xingyu Chen, Xiaoyuan Liu, Zhijie Wang, Junying Chen, Benyou Wang, <u>Zhaopeng Tu</u>*, Haitao Mi, and Dong Yu. <a href="http://dx.doi.org/10.13140/RG.2.2.33772.07043"><font color="#0080FF">The First Few Tokens Are All You Need: An Efficient and Effective Unsupervised Prefix Fine-Tuning Method for Reasoning Models</font></a>. <i>Preprint</i>. <a href="https://x.com/tuzhaopeng/status/1895420224683516413">X</a></li>	
	
    <li>Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Jiahao Xu, Tian Liang, Pinjia He, and <u>Zhaopeng Tu</u>. <a href="https://arxiv.org/abs/2407.09121"><font color="#0080FF">Refuse Whenever You Feel Unsafe: Improving Safety in LLMs via Decoupled Refusal Training</font></a>. <i>Preprint</i>. <a href="https://x.com/youliang_yuan/status/1812665889852121332">X</a></li>

    <li>Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Xing Wang, and <u>Zhaopeng Tu</u>. <a href="https://arxiv.org/abs/2301.08745"><font color="#0080FF">Is ChatGPT A Good Translator? Yes With GPT-4 As The Engine</font></a>. <i>Preprint</i>.</li>
    <li>Chenyang Lyu, Minghao Wu, Longyue Wang, Xinting Huang, Bingshuai Liu, Zefeng Du, Shuming Shi, and <u>Zhaopeng Tu</u>. <a href="https://arxiv.org/abs/2306.09093"><font color="#0080FF">Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration</font></a>. <i>Preprint</i>.</li>

    <!-- li>Wenxuan Wang, Juluan Shi, <u>Zhaopeng Tu</u>, Youliang Yuan, Jen-tse Huang, Wenxiang Jiao, and Michael R. Lyu. <a href="https://arxiv.org/abs/2401.00761"><font color="#0080FF">The Earth is Flat? Unveiling Factual Errors in Large Language Models</font></a>. <i>Preprint</i>.</li>       
    <li>Tian Liang, Zhiwei He, Jen-tse Huang, Wenxuan Wang, Wenxiang Jiao, Rui Wang, Yujiu Yang, <u>Zhaopeng Tu</u>, Shuming Shi, and Xing Wang. <a href="https://arxiv.org/abs/2310.20499"><font color="#0080FF">Leveraging Word Guessing Games to Assess the Intelligence of Large Language Models</font></a>. <i>Preprint</i>.</li>
    <li>Longyue Wang, Zefeng Du, DongHuai Liu, Deng Cai, Dian Yu, Haiyun Jiang, Yan Wang, Shuming Shi, and <u>Zhaopeng Tu</u>. <a href="https://openreview.net/forum?id=XIIynqbMXgR"><font color="#0080FF">GuoFeng: A Discourse-aware Evaluation Benchmark for Language Understanding, Translation and Generation</font></a>. <i>Preprint</i>.</li-->
	
<h3>2025</h3>
    <li>Zhiwei He, <u>Zhaopeng Tu</u>*, Xing Wang, Xingyu Chen, Zhijie Wang, Jiahao Xu, Tian Liang, Wenxiang Jiao, Zhuosheng Zhang, and Rui Wang. <a href="https://openreview.net/forum?id=GdXI5zCoAt"><font color="#0080FF">RaSA: Rank-Sharing Low-Rank Adaptation</font></a>. <i>ICLR 2025</i>.</li>
    <li>Jen-tse Huang, Eric John Li, Man Ho LAM, Tian Liang, Wenxuan Wang, Youliang Yuan, Wenxiang Jiao, Xing Wang, <u>Zhaopeng Tu</u>, and Michael Lyu. <a href="https://openreview.net/forum?id=DI4gW8viB6"><font color="#0080FF">Competing Large Language Models in Multi-Agent Gaming Environments</font></a>. <i>ICLR 2025</i>.</li>
    <li>Yongxin Guo, Zhenglin Cheng, Xiaoying Tang, <u>Zhaopeng Tu</u>, and Tao Lin. <a href="https://openreview.net/forum?id=T26f9z2rEe"><font color="#0080FF">Dynamic Mixture of Experts: An Auto-Tuning Approach for Efficient Transformer Models</font></a>. <i>ICLR 2025</i>.</li>


<h3>2024</h3>
    <li>Cunxiao Du, Jing Jiang, Xu Yuanchen, Jiawei Wu, Sicheng Yu, Yongqi Li, Shenggui Li, Kai Xu, Liqiang Nie, <u>Zhaopeng Tu</u>*, and Yang You. <a href="https://arxiv.org/abs/2402.02082"><font color="#0080FF">GliDe with a CaPE: A Low-Hassle Method to Accelerate Speculative Decoding</font></a>. <i>ICML 2024</i>.</li>
    
    <li>Jen-tse Huang, Man Ho LAM, Eric John Li, Shujie Ren, Wenxuan Wang, Wenxiang Jiao, <u>Zhaopeng Tu</u>, and Michael Lyu. <a href="https://openreview.net/forum?id=pwRVGRWtGg"><font color="#0080FF">Apathetic or Empathetic? Evaluating LLMs' Emotional Alignments with Humans</font></a>. <i>NeurIPS 2024</i>.</li>
    <li>Fanghua Ye, Mingming Yang, Jianhui Pang, Longyue Wang, Derek F. Wong, Emine Yilmaz, Shuming Shi, and <u>Zhaopeng Tu</u>. <a href="https://openreview.net/forum?id=L0oSfTroNE"><font color="#0080FF">Benchmarking LLMs via Uncertainty Quantification</font></a>. <i>NeurIPS 2024 (Datasets and Benchmarks Track)</i>.</li>
    <li>Hexuan Deng, Wenxiang Jiao, Xuebo Liu, Min Zhang, and <u>Zhaopeng Tu</u>. <a href="https://openreview.net/forum?id=NQLZoMHm6u"><font color="#0080FF">NewTerm: Benchmarking Real-Time New Terms for Large Language Models with Annual Updates</font></a>. <i>NeurIPS 2024 (Datasets and Benchmarks Track)</i>.</li>

    <li>Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Pinjia He, Shuming Shi, and <u>Zhaopeng Tu</u>. <a href="https://arxiv.org/abs/2308.06463"><font color="#0080FF">GPT-4 is too Smart to be Safe: Stealthy Chat with LLMs via Cipher</font></a>. <i>ICLR 2024</i>.</li>
    <li>Jen-tse Huang, Wenxuan Wang, Eric John Li, Man Ho LAM, Shujie Ren, Youliang Yuan, Wenxiang Jiao, <u>Zhaopeng Tu</u>, and Michael Lyu. <a href="https://openreview.net/forum?id=H3UayAQWoE"><font color="#0080FF">On the Humanity of Conversational AI: Evaluating the Psychological Portrayal of LLMs</font></a>. <i>ICLR 2024</i> (Oral, 1.2%).</li>

    <li>Wenxuan Wang, Wenxiang Jiao, Jingyuan Huang, Ruyi Dai, Jen-tse Huang, <u>Zhaopeng Tu</u>*, and Michael R Lyu. <a href="https://arxiv.org/abs/2310.12481"><font color="#0080FF">Not All Countries Celebrate Thanksgiving: On the Cultural Dominance in Large Language Models</font></a>. <i>ACL 2024</i>.</li>
    <li>Zhiwei He, Binglin Zhou, Hongkun Hao, Aiwei Liu, Xing Wang, <u>Zhaopeng Tu</u>, Zhuosheng Zhang, and Rui Wang. <font color="#0080FF">On the Cross-lingual Consistency of Text Watermark for Large Language Models</font>. <i>ACL 2024</i>.</li>
    <li>Wenxuan Wang, <u>Zhaopeng Tu</u>, Chang Chen, Youliang Yuan, Jen-tse Huang, Wenxiang Jiao, and Michael R Lyu. <a href="https://arxiv.org/abs/2310.00905"><font color="#0080FF">All Languages Matter: On the Multilingual Safety of Large Language Models</font></a>. <i>ACL 2024 (Findings)</i>.</li>
    <li>Zefeng Du, Wenxiang Jiao, Longyue Wang, Chenyang Lyu, Jianhui Pang, Leyang Cui, Kaiqiang Song, Derek F. Wong, Shuming Shi, and <u>Zhaopeng Tu</u>. <font color="#0080FF">Benchmarking and Improving Long-Text Translation with Large Language Models</font>. <i>ACL 2024 (Findings)</i>.</li>
    <li>Tian Liang, Xing Wang, Mingming Yang, Yujiu Yang, Shuming Shi, and <u>Zhaopeng Tu</u>. <font color="#0080FF">Addressing Entity Translation Problem via Translation Difficulty and Context Diversity</font>. <i>ACL 2024 (Findings)</i>.</li>
    <li>Zhengsheng Guo, Zhiwei He, Wenxiang Jiao, Xing Wang, Rui Wang, Kehai Chen, <u>Zhaopeng Tu</u>, Yong Xu, and Min Zhang. <font color="#0080FF">Unsupervised Sign Language Translation and Generation</font>. <i>ACL 2024 (Findings)</i>.</li>

    <li>Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Shuming Shi, and <u>Zhaopeng Tu</u>. <a href="https://arxiv.org/abs/2305.19118"><font color="#0080FF">Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate</font></a>. <i>EMNLP 2024</i>.</li>

    <li>Jianhui Pang, Fanghua Ye, Dian Yu, Derek F. Wong, Shuming Shi, <u>Zhaopeng Tu</u>, and Longyue Wang. <a href="https://arxiv.org/abs/2401.08350"><font color="#0080FF">Salute the Classic: Revisiting Challenges of Machine Translation in the Age of Large Language Models</font></a>. <i>TACL 2024</i>.</li>
    <li>Zhiwei He, Tian Liang, Wenxiang Jiao, Zhuosheng Zhang, Yujiu Yang, Rui Wang, <u>Zhaopeng Tu</u>*, Shuming Shi, and Xing Wang. <a href="https://arxiv.org/abs/2305.04118"><font color="#0080FF">Exploring Human-Like Translation Strategy with Large Language Models</font></a>. <i>TACL 2024</i>.</li>
    <li>Zhiwei He, Xing Wang, Wenxiang Jiao, Zhuosheng Zhang, Rui Wang, Shuming Shi, and <u>Zhaopeng Tu</u>. <a href="https://arxiv.org/abs/2401.12873"><font color="#0080FF">Improving Machine Translation with Human Feedback: An Exploration of Quality Estimation as a Reward Model</font></a>. <i>NAACL 2024</i>.</li>

    <li>Zhanyu Wang, Longyue Wang, Zhen Zhao, Minghao Wu, Chenyang Lyu, Huayang Li, Deng Cai, Luping Zhou, Shuming Shi, and <u>Zhaopeng Tu</u>. <a href="https://arxiv.org/abs/2311.16511"><font color="#0080FF">GPT4Video: A Unified Multimodal Large Language Model for lnstruction-Followed Understanding and Safety-Aware Generation</font></a>. <i>ACM MM 2024</i>. [<font color="#C4260B"><b>Best Paper Normination</b></font>]</li>
    
    <li>Bingshuai Liu, Longyue Wang, Chenyang Lyu, Yong Zhang, Jinsong Su, Shuming Shi, and <u>Zhaopeng Tu</u>*. <a href="https://arxiv.org/abs/2307.02971"><font color="#0080FF">On the Cultural Gap in Text-to-Image Generation</font></a>. <i>ECAI 2024</i>.</li>
    <li>Yan Liu, Longyue Wang, <u>Zhaopeng Tu</u>, and Deyi Xiong. <font color="#0080FF">On the Cultural Gap in Text-to-Image Generation</font>. <i>ECAI 2024</i>.</li>

    <li>Cunxiao Du, Hao Zhou, <u>Zhaopeng Tu</u>, and Jing Jiang. <font color="#0080FF">Revisiting the Markov Property for Machine Translation</font>. <i>EACL 2024 (Short, Findings)</i>.</li>

    <li>Wenxuan Wang, Wenxiang Jiao, Shuo Wang, <u>Zhaopeng Tu</u>*, and Michael Lyu. <a href="https://arxiv.org/abs/2205.10068"><font color="#0080FF">Understanding and Mitigating the Uncertainty in Zero-Shot Translation</font></a>. <i>Journal of TASLP, 2024</i>.</li>
    <li>Wenxuan Wang, Wenxiang Jiao, Jen-tse Huang, Shuming Shi, <u>Zhaopeng Tu</u>*, and Michael Lyu. <a href="https://openreview.net/forum?id=fQGjNpkGVf"><font color="#0080FF">On the Shortcut Learning in Multilingual Neural Machine Translation</font></a>. <i>Journal of NeuroComputing, 2024</i>.</li>



<h3>2023</h3>
    <li>Longyue Wang, Siyou Liu, Mingzhou Xu, Linfeng Song, Shuming Shi, and <u>Zhaopeng Tu</u>. <font color="#0080FF">A Survey on Zero Pronoun Translation</font>. <i>ACL 2023</i>. </li>
    <li>Zhihao Wang, Longyue Wang, Jinsong Su, Junfeng Yao, and <u>Zhaopeng Tu</u>. <font color="#0080FF">Revisiting Non-Autoregressive Translation at Scale</font>. <i>ACL 2023 (Findings)</i>. </li>
    
    <li>Longyue Wang, Chenyang Lyu, Tianbo Ji, Zhirui Zhang, Dian Yu, Shuming Shi, and <u>Zhaopeng Tu</u>. <a href="https://arxiv.org/pdf/2304.02210.pdf"><font color="#0080FF">Document-Level Machine Translation with Large Language Models</font></a>. <i>EMNLP 2023</i>.</li>
    <li>Wenxiang Jiao, Jen-tse Huang, Wenxuan Wang, Zhiwei He, Tian Liang, Xing Wang, Shuming Shi, and <u>Zhaopeng Tu</u>. <a href="https://arxiv.org/abs/2304.02426"><font color="#0080FF">ParroT: Translating During Chat Using Large Language Models</font></a>. <i>EMNLP 2023 (Findings)</i>.</li>
    <li>Jinhui Ye, Wenxiang Jiao, Xing Wang, <u>Zhaopeng Tu</u>, and Hui Xiong. <a href="https://openreview.net/forum?id=9Ax0pyaLgh"><font color="#0080FF">Cross-modality Data Augmentation for End-to-End Sign Language Translation</font></a>. <i>EMNLP 2023 (Findings)</i>.</li>


    <li>Kangjie Zheng, Longyue Wang, Zhihao Wang, Binqi Chen, Ming Zhang, and <u>Zhaopeng Tu</u>*. <font color="#0080FF">Towards A Unified Training for Levenshtein Transformer</font>. <i>ICASSP 2023</i>. </li>
    
    <li>Jinhui Ye, Wenxiang Jiao, Xing Wang, and <u>Zhaopeng Tu</u>. <a href="https://arxiv.org/abs/2210.07054"><font color="#0080FF">Scaling Back-Translation with Domain Text Generation for Sign Language Gloss Translation</font></a>. <i>EACL 2023</i>. </li>

    <li>Ante Wang, Qi Liu, Haitao Mi, Longyue Wang, <u>Zhaopeng Tu</u>, Jinsong Su, Dong Yu, and Linfeng Song. <font color="#0080FF">Search-Engine-augmented Dialogue Response Generation with Cheaply Supervised Query Production Corresponding</font>. <i>Journal of Artificial Intelligence, 2023</i>. </li>
    
    <li>Mingzhou Xu, Longyue Wang, Siyou Liu, Derek F Wong, Shuming Shi, and <u>Zhaopeng Tu</u>. <font color="#0080FF">A Benchmark Dataset and Evaluation Methodology for Chinese Zero Pronoun Translation</font>. <i>Journal of Language Resources and Evaluation, 2023</i>. </li>




<h3>2022</h3>

    <li>Liang Ding, Longyue Wang, Shuming Shi, Dacheng Tao, and <u>Zhaopeng Tu</u>*. <a href="https://openreview.net/forum?id=Zh4kJ4JOcHo"><font color="#0080FF">Redistributing Low-Frequency Words: Making the Most of Monolingual Data in Non-Autoregressive Translation</font></a>. <i>ACL 2022</i>. </li>
    <li>Zhiwei He, Xing Wang, Rui Wang, Shuming Shi, and <u>Zhaopeng Tu</u>. <a href="https://openreview.net/forum?id=WtlQ4cal2Y-"><font color="#0080FF">Bridging the Data Gap between Training and Inference for Unsupervised Neural Machine Translation</font></a>. <i>ACL 2022</i>. </li>
    <li>Wenxuan Wang, Wenxiang Jiao, Yongchang Hao, Xing Wang, Shuming Shi, <u>Zhaopeng Tu</u>*, and Michael Lyu. <a href="https://openreview.net/forum?id=ZkFuUac0Hc0"><font color="#0080FF">Understanding and Improving Sequence-to-Sequence Pretraining for Neural Machine Translation</font></a>. <i>ACL 2022</i>. </li>

    <li>Mingzhou Xu, Longyue Wang, Derek F. Wong, Hongye Liu, Linfeng Song, Lidia S. Chao, Shuming Shi, and <u>Zhaopeng Tu</u>. <font color="#0080FF">GuoFeng: A Benchmark for Zero Pronoun Recovery and Translation</font>. <i>EMNLP 2022</i>. </li>
    <li>Shuo Wang, Peng Li, Zhixing Tan, <u>Zhaopeng Tu</u>, Maosong Sun, and Yang Liu. <font color="#0080FF">A Template-based Method for Constrained Neural Machine Translation</font>. <i>EMNLP 2022</i>. </li>
    <li>Yifan Hou, Wenxiang Jiao, Meizhen Liu, Carl Allen, <u>Zhaopeng Tu</u>, and Mrinmaya Sachan. <font color="#0080FF">Adapters for Enhanced Modeling of Multilingual Knowledge and Text</font>. <i>EMNLP 2022 (Findings)</i>. [<font color="#C4260B"><b>Best Paper of MRL Workshop</b></font>]</li>

    <li>Cunxiao Du, <u>Zhaopeng Tu</u>*, Longyue Wang, and Jing Jiang. <a href=""><font color="#0080FF">ngram-OAXE: Phrase-Based Order-Agnostic Cross Entropy for Non-Autoregressive Machine Translation</font></a>. <i>COLING 2022</i>. </li>
    
    <li>Wenxiang Jiao, Xing Wang, Shilin He, <u>Zhaopeng Tu</u>, Irwin King, and Michael Lyu. <a href=""><font color="#0080FF">Exploiting Inactive Examples for Natural Language Generation with Data Rejuvenation</font></a>. <i>Journal of TASLP, 2022</i>. </li>
    
    <li>Xinwei Geng, Longyue Wang, Xing Wang, Mingtao Yang, Xiaocheng Feng, Bing Qin, and <u>Zhaopeng Tu</u>. <a href=""><font color="#0080FF">Learning to Refine Source Representations for Neural Machine Translation</font></a>. <i>International Journal of Machine Learning and Cybernetics, 2022</i>. </li>


<h3>2021</h3>
    <li>Cunxiao Du, <u>Zhaopeng Tu</u>*, and Jing Jiang. <a href="https://arxiv.org/abs/2106.05093"><font color="#0080FF">Order-Agnostic Cross Entropy for Non-Autoregressive Machine Translation</font></a>. <i>ICML 2021</i> (Oral, 3%). </li>

    <li>Liang Ding, Longyue Wang, Xuebo Liu, Derek F. Wong, Dacheng Tao, and <u>Zhaopeng Tu</u>. <a href="https://arxiv.org/abs/2106.00903"><font color="#0080FF">Rejuvenating Low-Frequency Words: Making the Most of Parallel Data in Non-Autoregressive Translation</font></a>. <i>ACL 2021</i>. </li>
    <li>Wenxiang Jiao, Xing Wang, <u>Zhaopeng Tu</u>, Shuming Shi, Michael R. Lyu, and Irwin King. <a href="https://arxiv.org/abs/2106.00941"><font color="#0080FF">Self-Training Sampling with Monolingual Data Uncertainty for Neural Machine Translation</font></a>. <i>ACL 2021</i>. </li>

    <li>Shuo Wang, <u>Zhaopeng Tu</u>, Zhixing Tan, Shuming Shi, Maosong Sun, Yang Liu. <a href="https://arxiv.org/abs/2106.03297"><font color="#0080FF">On the Language Coverage Bias for Neural Machine Translation</font></a>. <i>ACL 2021 (Findings)</i>. </li>
    <li>Xuebo Liu, Longyue Wang, Derek F. Wong, Liang Ding, Lidia S. Chao, Shuming Shi, and <u>Zhaopeng Tu</u>. <a href="https://aclanthology.org/2021.findings-acl.373/"><font color="#0080FF">On the Copying Behaviors of Pre-Training for Neural Machine Translation</font></a>. <i>ACL 2021 (Findings)</i>. </li>
    <li>Liang Ding, Longyue Wang, Xuebo Liu, Derek F. Wong, Dacheng Tao, and <u>Zhaopeng Tu</u>. <a href="https://arxiv.org/abs/2106.05546"><font color="#0080FF">Progressive Multi-Granularity Training for Non-Autoregressive Translation</font></a>. <i>ACL 2021 (Findings, Short)</i>. </li>

    <li>Liang Ding, Longyue Wang, Xuebo Liu, Derek F. Wong, Dacheng Tao, and <u>Zhaopeng Tu</u>. <a href="https://openreview.net/forum?id=ZTFeSBIX9C"><font color="#0080FF">Understanding and Improving Lexical Choice in Non-Autoregressive Translation</font></a>. <i>ICLR 2021</i>. </li>
    <li>Xuebo Liu, Longyue Wang, Derek F. Wong, Liang Ding, Lidia S. Chao, and <u>Zhaopeng Tu</u>. <a href="https://openreview.net/forum?id=n1HD8M6WGn"><font color="#0080FF">Understanding and Improving Encoder Layer Fusion in Sequence-to-Sequence Learning</font></a>. <i>ICLR 2021</i>. </li>
    
    <li>Jie Hao, Linfeng Song, Liwei Wang, Kun Xu, <u>Zhaopeng Tu</u>, and Dong Yu. <a href="https://aclanthology.org/2021.emnlp-main.402/"><font color="#0080FF">RAST: Domain-Robust Dialogue Rewriting as Sequence Tagging</font></a>. <i>EMNLP 2021</i>. </li>
    <li>Xuebo Liu, Longyue Wang, Derek F Wong, Liang Ding, Lidia S Chao, Shuming Shi, and <u>Zhaopeng Tu</u>. <a href="https://arxiv.org/abs/2110.01811"><font color="#0080FF">On the Complementarity between Pre-Training and Back-Translation for Neural Machine Translation</font></a>. <i>EMNLP 2021 (Findings, Short)</i>. </li>
    
    <li>Yongchang Hao, Shilin He, Wenxiang Jiao, <u>Zhaopeng Tu</u>, Michael Lyu, and Xing Wang. <a href="https://arxiv.org/abs/2010.12868"><font color="#0080FF">Multi-Task Learning with Shared Encoder for Non-Autoregressive Machine Translation</font></a>. <i>NAACL 2021 (Short)</i>. </li>
    
    <li>Jian Li, Xing Wang, <u>Zhaopeng Tu</u>*, and Michael R Lyu. <a href="https://www.sciencedirect.com/science/article/pii/S0925231221005725"><font color="#0080FF">On the Diversity of Multi-Head Attention</font></a>. <i>Journal of Neurocomputing, 2021</i>. </li>
    <li>Baosong Yang, Longyue Wang, Derek F Wong, Shuming Shi, and <u>Zhaopeng Tu</u>*. <a href="https://www.sciencedirect.com/science/article/pii/S0925231221009048"><font color="#0080FF">Context-Aware Self-Attention Networks for Natural Language Processing</font></a>. <i>Journal of Neurocomputing, 2021</i>. </li>
    <li>Xintong Li, Lemao Liu, <u>Zhaopeng Tu</u>, Guanlin Li, Shuming Shi, Max Q-H Meng. <a href=""><font color="#0080FF">Attending From Foresight: A Novel Attention Mechanism for Neural Machine Translation</font></a>. <i>Journal of TASLP, 2021</i>. </li>

    <li>Shuo Wang, <u>Zhaopeng Tu</u>, Zhixing Tan, Wenxuan Wang, Maosong Sun, and Yang Liu. <a href="https://arxiv.org/abs/2106.13627"><font color="#0080FF">Language Models are Good Translators</font></a>. <i>arXiv 2021</i>. </li>


<h3>2020</h3>
    <li>Shuo Wang, <u>Zhaopeng Tu</u>, Shuming Shi, and Yang Liu. <a href="https://aclanthology.org/2020.acl-main.278/"><font color="#0080FF">On the Inference Calibration of Neural Machine Translation</font></a>. <i>ACL 2020</i>. </li>
    <li>Xinwei Geng, Longyue Wang, Xing Wang, Bing Qin, Ting Liu, and <u>Zhaopeng Tu</u>. <a href="https://aclanthology.org/2020.acl-main.269/"><font color="#0080FF">How Does Selective Mechanism Improve Self-Attention Networks?</font></a> <i>ACL 2020</i>. </li>
    
    <li>Wenxiang Jiao, Xing Wang, Shilin He, Irwin King, Michael Lyu, and <u>Zhaopeng Tu</u>. <a href="https://aclanthology.org/2020.emnlp-main.176/"><font color="#0080FF">Data Rejuvenation: Exploiting Inactive Training Examples for Neural Machine Translation</font></a>. <i>EMNLP 2020</i>.</li>
    <li>Yilin Yang, Longyue Wang, Shuming Shi, Prasad Tadepalli, Stefan Lee, and <u>Zhaopeng Tu</u>. <a href="https://aclanthology.org/2020.findings-emnlp.432/"><font color="#0080FF">On the Sub-Layer Functionalities of Transformer Decoder</font></a>. <i>EMNLP 2020 (Findings)</i>.</li>
    <li>Yong Wang, Longyue Wang, Victor O.K. Li, and <u>Zhaopeng Tu</u>. <a href="https://aclanthology.org/2020.emnlp-main.78/"><font color="#0080FF">On the Sparsity of Neural Machine Translation Models</font></a>. <i>EMNLP 2020 (Short)</i>.</li>

    <li>Wenxuan Wang and <u>Zhaopeng Tu</u>*. <a href="https://aclanthology.org/2020.coling-main.529/"><font color="#0080FF">Rethinking the Value of Transformer Components</font></a>. <i>COLING 2020</i>.</li>
    <li>Deyu Zhou, Shuangzhi Wu, Qing Wang, Jun Xie, <u>Zhaopeng Tu</u>, and Mu Li. <a href="https://aclanthology.org/2020.coling-main.288/"><font color="#0080FF">Emotion Classification by Jointly Learning to Lexiconize and Classify</font></a>. <i>COLING 2020</i>.</li>
    <li>Qintong Li, Hongshen Chen, Zhaochun Ren, Pengjie Ren, <u>Zhaopeng Tu</u>, and Zhumin Chen. <a href="https://aclanthology.org/2020.coling-main.394/"><font color="#0080FF">EmpDG: Multi-resolution Interactive Empathetic Dialogue Generation</font></a>. <i>COLING 2020</i>.</li>
    <li>Liang Ding, Longyue Wang, Di Wu, Dacheng Tao, and <u>Zhaopeng Tu</u>. <a href="https://aclanthology.org/2020.coling-main.389/"><font color="#0080FF">Context-Aware Cross-Attention for Non-Autoregressive Translation</font></a>. <i>COLING 2020 (Short)</i>.</li>

    <li>Jian Li, Xing Wang, Baosong Yang, Shuming Shi, Michael R. Lyu, and <u>Zhaopeng Tu</u>*. <a href="https://arxiv.org/abs/1911.09877"><font color="#0080FF">Neuron Interaction Based Representation Composition for Neural Machine Translation</font></a>. <i>AAAI 2020</i>. </li>
    <li>Yong Wang, Longyue Wang, Shuming Shi, Victor O.K. Li, and <u>Zhaopeng Tu</u>. <a href="https://arxiv.org/abs/1911.09912"><font color="#0080FF">Go From the General to the Particular: Multi-Domain Translation with Domain Transformation Networks</font></a>. <i>AAAI 2020</i>.</li>
    <li>Tianxiang Zhao, Lemao Liu, Guoping Huang, <u>Zhaopeng Tu</u>, Huayang Li, Yingling Liu, Guiquan Liu, and Shuming Shi. <font color="#0080FF">Balancing Quality and Human Involvement: An Effective Approach to Interactive Neural Machine Translation</font>. <i>AAAI 2020</i>.</li>
    
    <li>Yongquan He, Zhihan Wang, Peng Zhang, <u>Zhaopeng Tu</u>, and Zhaochun Ren. <font color="#0080FF">VN Network: Embedding Newly Emerging Entities with Virtual Neighbors</font>. <i>CIKM 2020</i>.
    <li>Jinhuan Liu, Xuemeng Song, Zhaochun Ren, Liqiang Nie, <u>Zhaopeng Tu</u>, and Jun Ma. <font color="#0080FF">Auxiliary Template-Enhanced Generative Compatibility Modeling</font>. <i>IJCAI 2020</i>.</li>
    <li>Chuan Meng, Pengjie Ren, Zhumin Chen, Weiwei Sun, Zhaochun Ren, <u>Zhaopeng Tu</u>, and Maarten de Rijke. <font color="#0080FF">DukeNet: A Dual Knowledge Interaction Network for Knowledge-Grounded Conversation</font>. <i>SIGIR 2020</i>.</li>


<h3>2019</h3>
    <li>Baosong Yang, Longyue Wang, Derek F. Wong, Lidia S. Chao, and <u>Zhaopeng Tu</u>*. <a href="https://arxiv.org/abs/1906.00592"><font color="#0080FF">Assessing the Ability of Self-Attention Networks to Learn Word Order</font></a>. <i>ACL 2019</i>. </li>
    <li>Xing Wang, <u>Zhaopeng Tu</u>, Longyue Wang, and Shuming Shi. <a href="https://arxiv.org/abs/1906.01268"><font color="#0080FF">Exploiting Sentential Context for Neural Machine Translation</font></a>. <i>ACL 2019 (Short)</i>. </li>

    <li>Shilin He, <u>Zhaopeng Tu</u>*, Xing Wang, Longyue Wang, Michael R. Lyu, and Shuming Shi. <a href="https://arxiv.org/abs/1909.00326"><font color="#0080FF">Towards Understanding Neural Machine Translation with Word Importance</font></a>. <i>EMNLP 2019</i>. </li>
    <li>Longyue Wang, <u>Zhaopeng Tu</u>, Xing Wang, and Shuming Shi. <a href="https://arxiv.org/abs/1909.00369"><font color="#0080FF">One Model to Learn Both: Zero Pronoun Prediction and Translation</font></a>. <i>EMNLP 2019</i>. </li>
    <li>Jie Hao, Xing Wang, Shuming Shi, Jinfeng Zhang, and <u>Zhaopeng Tu</u>. <a href="https://arxiv.org/abs/1909.02222"><font color="#0080FF">Multi-Granularity Self-Attention for Neural Machine Translation</font></a>. <i>EMNLP 2019</i>.</li>
    <li>Zaixiang Zheng, Shujian Huang, <u>Zhaopeng Tu</u>, Xinyu Dai, and Jiajun Chen. <a href="https://arxiv.org/abs/1904.09646"><font color="#0080FF">Dynamic Past and Future for Neural Machine Translation</font></a>. <i>EMNLP 2019</i>.</li>
    <li>Deng Cai, Yan Wang, Wei Bi, <u>Zhaopeng Tu</u>, Xiaojiang Liu, and Shuming Shi. <font color="#0080FF">Retrieval-guided Dialogue Response Generation via a Matching-to-Generation Framework</font>. <i>EMNLP 2019</i>.</li>
    <li>Xing Wang, <u>Zhaopeng Tu</u>, Longyue Wang, and Shuming Shi. <a href="https://arxiv.org/abs/1909.00383"><font color="#0080FF">Self-Attention Networks with Structural Position Encoding</font></a>. <i>EMNLP 2019 (Short)</i>. </li>
    <li>Jie Hao, Xing Wang, Shuming Shi, Jinfeng Zhang, and <u>Zhaopeng Tu</u>. <a href="https://arxiv.org/abs/1909.01562"><font color="#0080FF">Towards Better Modeling Hierarchical Structure for Self-Attention with Ordered Neurons</font></a>. <i>EMNLP 2019 (Short)</i>.</li>
    
    <li>Jian Li, Baosong Yang, Zi-Yi Dou, Xing Wang, Michael R. Lyu, and <u>Zhaopeng Tu</u>*. <a href="https://arxiv.org/abs/1904.03100"><font color="#0080FF">Information Aggregation for Multi-Head Attention with Routing-by-Agreement</font></a>. <i>NAACL 2019</i>. </li>    
    <li>Jie Hao, Xing Wang, Baosong Yang, Longyue Wang, Jinfeng Zhang, and <u>Zhaopeng Tu</u>*. <a href="https://arxiv.org/abs/1904.03092"><font color="#0080FF">Modeling Recurrence for Transformer</font></a>. <i>NAACL 2019</i>. </li>
    <li>Deng Cai, Yan Wang, Wei Bi, <u>Zhaopeng Tu</u>, Xiaojiang Liu, Wai Lam, and Shuming Shi. <a href="https://arxiv.org/abs/1809.05296"><font color="#0080FF">Skeleton-to-Response: Dialogue Generation Guided by Retrieval Memory</font></a>. <i>NAACL 2019</i>. </li>
    
    <li>Baosong Yang, Longyue Wang, Derek F. Wong, Lidia S. Chao, and <u>Zhaopeng Tu</u>*. <a href="https://arxiv.org/abs/1904.03107"><font color="#0080FF">Convolutional Self-Attention Networks</font></a>. <i>NAACL 2019 (Short)</i>. </li>
    <li>Xiang Kong, <u>Zhaopeng Tu</u>*, Shuming Shi, Eduard Hovy, and Tong Zhang. <a href="https://arxiv.org/abs/1811.08541"><font color="#0080FF">Neural Machine Translation with Adequacy-Oriented Learning</font></a>. <i>AAAI 2019</i>. </li>
    <li>Zi-Yi Dou, <u>Zhaopeng Tu</u>*, Xing Wang, Longyue Wang, Shuming Shi, and Tong Zhang. <a href="https://arxiv.org/abs/1902.05770"><font color="#0080FF">Dynamic Layer Aggregation for Neural Machine Translation with Routing-by-Agreement</font></a>. <i>AAAI 2019</i>. </li>
    <li>Baosong Yang, Jian Li, Derek F. Wong, Lidia S. Chao, Xing Wang, and <u>Zhaopeng Tu</u>*. <a href="https://arxiv.org/abs/1902.05766"><font color="#0080FF">Context-Aware Self-Attention Networks</font></a>. <i>AAAI 2019</i>. </li>
    
    <li>Zi-Yi Dou, Xing Wang, Shuming Shi, and <u>Zhaopeng Tu</u>*. <a href="https://www.sciencedirect.com/science/article/abs/pii/S0925231219317692?via%3Dihub"><font color="#0080FF">Exploiting Deep Representations for Natural Language Processing</font></a>. <i>Journal of NeuroComputing, 2019</i>.</li>


<h3>2018</h3>
    <li>Yong Cheng, <u>Zhaopeng Tu</u>, Fandong Meng, Junjie Zhai, and Yang Liu. <a href="https://arxiv.org/abs/1805.06130"><font color="#0080FF">Towards Robust Neural Machine Translation</font></a>. <i>ACL 2018</i>. </li>
    
    <li>Zi-Yi Dou, <u>Zhaopeng Tu</u>*, Xing Wang, Shuming Shi, and Tong Zhang. <a href="https://arxiv.org/abs/1810.10181"><font color="#0080FF">Exploiting Deep Representations for Neural Machine Translation</font></a>. <i>EMNLP 2018</i>. </li>
    <li>Baosong Yang, <u>Zhaopeng Tu</u>*, Derek F. Wong, Fandong Meng, Lidia S. Chao, and Tong Zhang. <a href="https://arxiv.org/abs/1810.10182"><font color="#0080FF">Modeling Localness for Self-Attention Networks</font></a>. <i>EMNLP 2018</i>. </li>
    <li>Jian Li, <u>Zhaopeng Tu</u>*, Baosong Yang, Michael R. Lyu, and Tong Zhang. <a href="https://arxiv.org/abs/1810.10183"><font color="#0080FF">Multi-Head Attention with Disagreement Regularization</font></a>. <i>EMNLP 2018 (Short)</i>. </li>
    <li>Longyue Wang, <u>Zhaopeng Tu</u>*, Andy Way, and Qun Liu. <a href="https://arxiv.org/abs/1801.03257"><font color="#0080FF">Learning to Jointly Translate and Predict Dropped Pronouns with a Shared Reconstruction Mechanism</font></a>. <i>EMNLP 2018 (Short)</i>. </li> 
    
    <li>Fandong Meng, <u>Zhaopeng Tu</u>, Yong Cheng, Haiyang Wu, Junjie Zhai, Yuekui Yang, and Di Wang. <a href="https://arxiv.org/abs/1806.11249"><font color="#0080FF">Neural Machine Translation with Key-Value Memory-Augmented Attention</font></a>. <i>IJCAI 2018</i>. </li>

    <li>Longyue Wang, <u>Zhaopeng Tu</u>*, Shuming Shi, Tong Zhang, Yvette Graham, and Qun Liu. <a href="https://arxiv.org/abs/1801.03257"><font color="#0080FF">Translating Pro-Drop Languages with Reconstruction Models</font></a>. <i>AAAI 2018</i>. </li>

    <li>Xintong Li, Lemao Liu, <u>Zhaopeng Tu</u>, Shuming Shi, and Max Meng. <a href="http://www.aclweb.org/anthology/N18-1125"><font color="#0080FF">Target Foresight Based Attention for Neural Machine Translation</font></a>. <i>NAACL 2018</i>. </li>

    <li><u>Zhaopeng Tu</u>, Yang Liu, Shuming Shi, and Tong Zhang. <a href="https://arxiv.org/abs/1711.09367"><font color="#0080FF">Learning to Remember Translation History with a Continuous Cache</font></a>. <i>Transactions of the Association for Computational Linguistics (TACL), 2018</i>. </li> 
    <li>Zaixiang Zheng, Hao Zhou, Shujian Huang, Lili Mou, Xinyu Dai, Jiajun Chen, and <u>Zhaopeng Tu</u>. <a href="https://arxiv.org/abs/1711.09502"><font color="#0080FF">Modeling Past and Future for Neural Machine Translation</font></a>. <i>Transactions of the Association for Computational Linguistics (TACL), 2018</i>. </li>

    <li>Xing Wang, <u>Zhaopeng Tu</u>, and Min Zhang. <font color="#0080FF">Incorporating Statistical Machine Translation Word Knowledge into Neural Machine Translation</font>. <i>IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP), 2018</i>. </li>
    <li><u>Zhaopeng Tu</u>, Yong Jiang, Xiaojiang Liu, Lei Shu, and Shuming Shi. <a href="https://arxiv.org/abs/1804.07942"><font color="#0080FF">Generative Stock Question Answering</font></a>. <i>arXiv:1804.07942</i>. [<a href="http://ai.tencent.com/ailab/nlp/data/stockQA.tar.gz">data</a>]</li>


<h3>2017</h3>
        <li>Junhui Li, Deyi Xiong, <u>Zhaopeng Tu</u>, Muhua Zhu, and Guodong Zhou. <a href="papers/acl2017_source_syntax.pdf"><font color="#0080FF">Modeling Source Syntax for Neural Machine Translation</font></a>. <i>ACL 2017</i>. </li>
        <li>Hao Zhou, <u>Zhaopeng Tu</u>, Shujian Huang, Xiaohua Liu, Hang Li, and Jiajun Chen. <a href="papers/acl2017_chunk_decoder.pdf"><font color="#0080FF">Chunk-Based Bi-Scale Decoder for Neural Machine Translation</font></a>. <i>ACL 2017 (Short)</i>. </li>

        <li>Xing Wang, <u>Zhaopeng Tu</u>, Deyi Xiong, and Min Zhang. <a href="https://arxiv.org/abs/1708.01980"><font color="#0080FF">Translating Phrases in Neural Machine Translation</font></a>. <i>EMNLP 2017</i>. </li>
        <li>Longyue Wang, <u>Zhaopeng Tu</u>*, Andy Way, and Qun Liu. <a href="https://arxiv.org/abs/1704.04347"><font color="#0080FF">Exploiting Cross-Sentence Context for Neural Machine Translation</font></a>. <i>EMNLP 2017 (Short)</i>. </li>

        <li><u>Zhaopeng Tu</u>, Yang Liu, Lifeng Shang, Xiaohua Liu, and Hang Li. <a href="https://arxiv.org/abs/1611.01874"><font color="#0080FF">Neural Machine Translation with Reconstruction</font></a>. <i>AAAI 2017</i>. [<a href="https://github.com/tuzhaopeng/NMT">code</a>]</li>
        <li>Xing Wang, Zhengdong Lu, <u>Zhaopeng Tu</u>, Hang Li, Deyi Xiong, and Min Zhang. <a href="https://arxiv.org/abs/1610.05150"><font color="#0080FF">Neural Machine Translation Advised by Statistical Machine Translation</font></a>. <i>AAAI 2017</i>. </li>

        <li><u>Zhaopeng Tu</u>, Yang Liu, Zhengdong Lu, Xiaohua Liu, and Hang Li. <a href="http://arxiv.org/abs/1608.06043"><font color="#0080FF">Context Gates for Neural Machine Translation</font></a>. <i>Transactions of the Association for Computational Linguistics (TACL), 2017</i>. [<a href="https://github.com/tuzhaopeng/NMT">code</a>]</li>


<h3>2016</h3>
    <li><u>Zhaopeng Tu</u>, Zhengdong Lu, Yang Liu, Xiaohua Liu, and Hang Li. <a href="http://arxiv.org/abs/1601.04811"><font color="#0080FF">Modeling Coverage for Neural Machine Translation</font></a>. <i>ACL 2016</i>. [<a href="https://github.com/tuzhaopeng/NMT-Coverage">code</a>]</li>
    
    <li>Longyue Wang, <u>Zhaopeng Tu</u>, Xiaojun Zhang, Hang Li, Andy Way, and Qun Liu. <a href="papers/naacl2016_dropped_pronoun.pdf"><font color="#0080FF">A Novel Approach for Dropped Pronoun Translation</font></a>. <i>NAACL 2016</i>. </li>
    <li>Longyue Wang, Xiaojun Zhang, <u>Zhaopeng Tu</u>, Hang Li, and Qun Liu. <a href="papers/icassp2016_dropped_pronoun.pdf"><font color="#0080FF">Dropped Pronoun Generation for Dialogue Machine Translation</font></a>. <i>ICASSP 2016</i>.</li>
    <li>Baishakhi Ray, Vincent Hellendoorn, Saheel Godhane, <u>Zhaopeng Tu</u>, Alberto Bacchelli, and Premkumar Devanbu. <a href="papers/icse2016_naturalness.pdf"><font color="#0080FF">On the "Naturalness" of Buggy Code</font></a>. <i>ICSE 2016</i>. </li>
    <li>Anh Nguyen, <u>Zhaopeng Tu</u>, and Tien Nguyen. <font color="#0080FF">Do Contexts Help in Phrase-based, Statistical Source Code Migration?</font> <i>ICSME 2016</i>.</li> 
    <li>Longyue Wang, Xiaojun Zhang, <u>Zhaopeng Tu</u>, Andy Way, and Qun Liu. <a href="papers/lrec2016_discourse_corpora.pdf"><font color="#0080FF">The Automatic Construction of Discourse Corpus for Dialogue Translation</font></a>. <i>LREC 2016</i>. </li>
    
    <li>Longyue Wang, <u>Zhaopeng Tu</u>, Xiaojun Zhang, Siyou Liu, Hang Li, Andy Way, and Qun Liu. <a href="papers/jmt2016_prodrop.pdf"><font color="#0080FF">A Novel and Robust Approach for Pro-Drop Language Translation</font></a>. <i>Journal of Machine Translation, 2016</i>. </li>


<h3>2010 ~ 2015</h3>
    <li><u>Zhaopeng Tu</u>, Baotian Hu, Zhengdong Lu, and Hang Li. <a href="http://arxiv.org/abs/1503.02357"><font color="#0080FF">Context-Dependent Translation Selection Using Convolutional Neural Network</font></a>. <i>arXiv 2015</i>. (Short version is accepted by <i>ACL 2015 (Short)</i>)</li>
    <li>Christine Franks, <u>Zhaopeng Tu</u>, Prem Devanbu, and Vincent Hellendoorn. <a href="papers/icse2015_cacheca.pdf"><font color="#0080FF">CACHECA: A Cache Language Model Based Code Suggestion Tool</font></a>. <i>ICSE 2015 - Demonstration Track</i>. (Demo of the &quot;localness&quot; work)</li>
	<li>Fandong Meng, Zhengdong Lu, <u>Zhaopeng Tu</u>, Hang Li, and Qun Liu. <a href="http://arxiv.org/abs/1506.06442"><font color="#0080FF">A Deep Memory-based Architecture for Sequence-to-Sequence Learning</font></a>. <i>arXiv 2015</i>.</li>

    <li><u>Zhaopeng Tu</u>, Zhendong Su, and Prem Devanbu. <a href="papers/fse2014_localness.pdf"><font color="#0080FF">On the Localness of Software</font></a>. <i>FSE 2014</i>. [<a href="https://bitbucket.org/tuzhaopeng/cachelm_for_code_suggestion">code</a>] [<a href="data/fse2014_code_corpus.tar.gz">data</a>]</li>

	<li>Qun Liu, <u>Zhaopeng Tu</u>, and Shouxun Lin. <a href="papers/acl2013_weighted_alignment_hypergraph.pdf"><font color="#0080FF">A Novel Graph-based Compact Representation of Word Alignment</font></a>. <i>ACL 2013 (Short)</i>.</li>
	<li><u>Zhaopeng Tu</u>, Yifan He, Jennifer Foster, Josef van Genabith, Qun Liu, and Shouxun Lin. <a href="papers/acl2012_convolution_kernels.pdf"><font color="#0080FF">Identifying High-Impact Sub-Structures for Convolution Kernels in Document-level Sentiment Classification</font></a>. <i>ACL 2012 (Short)</i>.</li>
	<li>Junhui Li, <u>Zhaopeng Tu</u>, Guodong Zhou, and Josef van Genabith. <a href="papers/acl2012_head-driven_hpb.pdf"><font color="#0080FF">Head-Driven Hierarchical Phrase-based Translation</font></a>. <i>ACL 2012 (Short)</i>.</li>
	<li><u>Zhaopeng Tu</u>, Yang Liu, Yifan He, Josef van Genabith, Qun Liu, and Shouxun Lin. <a href="papers/coling2012_multiple_alignments.pdf"><font color="#0080FF">Combining Multiple Alignments to Improve Machine Translation</font></a>. <i>COLING 2012</i>. [<a href="data/coling2012_hand-aligned_dataset.tar.gz">data</a>]</li>
	<li>Junhui Li, <u>Zhaopeng Tu</u>, Guodong Zhou, and Josef van Genabith. <a href="papers/wmt2012_head_information_for_hpb.pdf"><font color="#0080FF">Using Syntactic Head Information in Hierarchical Phrase-based Translation</font></a>. <i>WMT 2012</i>.</li>
	<li><u>Zhaopeng Tu</u>, Wenbin Jiang, Qun Liu, and Shouxun Lin. <a href="papers/nlp2012_dependency_forest_for_sa.pdf"><font color="#0080FF">Dependency Forest for Sentiment Analysis</font></a>. <i>NLP&amp;CC 2012</i>.</li>
	<li><u>Zhaopeng Tu</u>, Yang Liu, Qun Liu, and Shouxun Lin. <a href="papers/ijcnlp2011_hierarchical_wam.pdf"><font color="#0080FF">Extracting Hierarchical Rules from a Weighted Alignment Matrix</font></a>. <i>IJCNLP 2011</i>.</li>
	<li><u>Zhaopeng Tu</u>, Yang Liu, Young-Sook Hwang, Qun Liu, and Shouxun Lin. <a href="papers/coling2010_dependency_forest.pdf"><font color="#0080FF">Dependency Forest for Statistical Machine Translation</font></a>. <i>COLING 2010</i>.</li>
</ol>

<br/>


</body>
</html>
